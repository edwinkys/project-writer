{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit382pyenv1518160d034e4ba6ab51bd99c9504900",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions\n",
    "\n",
    "This notebook contains the functions that is needed for the production model for the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Notebook\n",
    "Run all of the command below to start the notebook training session of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to kill the runtime in Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to install necessary file in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "\n",
    "# !pip install gpt_2_simple\n",
    "# !pip install wikipedia\n",
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/datasets\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/samples\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to download the model in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt_2_simple as gpt2\n",
    "# gpt2.download_gpt2(model_name='355M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf\n",
    "import wikipedia\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scrapping\n",
    "\n",
    "This function is used for scrapping all the text that is contains within a certain website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract():\n",
    "    def __init__(self, return_as_file=True):\n",
    "        self.return_as_file = return_as_file  # Export the dataset as a file\n",
    "\n",
    "    def remove_blank_lines(self, paragraph):\n",
    "        '''\n",
    "\n",
    "        Function to remove extra blank lines in a paragraphs.\n",
    "\n",
    "        @paragraph: A list of string\n",
    "\n",
    "        return: A paragraph without extra blank lines\n",
    "\n",
    "        '''\n",
    "\n",
    "        lines = paragraph.split('\\n')\n",
    "\n",
    "        non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "\n",
    "        string_without_empty_lines = ''\n",
    "        for line in non_empty_lines:\n",
    "            string_without_empty_lines += line + '\\n'\n",
    "\n",
    "        return string_without_empty_lines\n",
    "\n",
    "    def extract_from_investopedia(self, urls):\n",
    "        '''\n",
    "\n",
    "        Function to extract the text in the list of urls of Investopedia.\n",
    "\n",
    "        @urls: List of investopedia urls\n",
    "\n",
    "        return: String containing text from urls\n",
    "\n",
    "        '''\n",
    "\n",
    "        # List of elements containing text\n",
    "        elements = [\n",
    "            'article'\n",
    "        ]\n",
    "\n",
    "        # List of elements to delete\n",
    "        delete_elements = [\n",
    "            'header',\n",
    "            'span',\n",
    "            'footer'\n",
    "        ]\n",
    "\n",
    "        # Initialize string container\n",
    "        texts = ''\n",
    "\n",
    "        # Loop over the urls\n",
    "        for url in urls:\n",
    "            page = urlopen(url).read()\n",
    "\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "            # Delete some elements\n",
    "            for element in soup(delete_elements):\n",
    "                element.decompose()\n",
    "\n",
    "            # Remove useless div\n",
    "            for div in soup.find_all('div', ['breadcrumbs']): \n",
    "                div.decompose()\n",
    "\n",
    "            list_text_tags = soup.find_all(elements)\n",
    "\n",
    "            for tag in list_text_tags:\n",
    "                text = tag.text\n",
    "\n",
    "                # Remove extra spaces\n",
    "                text = text.strip()\n",
    "\n",
    "                # Add the text to the container\n",
    "                texts += text\n",
    "\n",
    "        # Remove extra spaces\n",
    "        texts = self.remove_blank_lines(texts)\n",
    "        texts = texts.strip()\n",
    "\n",
    "        if self.return_as_file:\n",
    "            filename = 'investopedia_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())  # Export a text file\n",
    "            path = r'./datasets/' + filename\n",
    "\n",
    "            # Write the text file in the datasets folder\n",
    "            with open(path, 'w') as f:\n",
    "                f.write(texts)\n",
    "\n",
    "        else:\n",
    "            return texts\n",
    "\n",
    "    def extract_from_wikipedia(self, titles):\n",
    "        '''\n",
    "\n",
    "        Function to extract text from wikipedia.\n",
    "\n",
    "        @title: The list title of the Wikipedia article\n",
    "\n",
    "        return: A string containing the text from a wikipedia\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Initialize the container\n",
    "        texts = ''\n",
    "\n",
    "        for title in titles:\n",
    "            # Get the wikipedia page\n",
    "            page = wikipedia.page(title)\n",
    "\n",
    "            # Extract the text\n",
    "            text = page.content\n",
    "\n",
    "            # Clean text\n",
    "            text = re.sub(r'==.*?==+', '', text)\n",
    "\n",
    "            texts += text\n",
    "        \n",
    "        texts = self.remove_blank_lines(texts)\n",
    "\n",
    "        if self.return_as_file:\n",
    "            filename = 'wikipedia_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())  # Export a text file\n",
    "            path = r'./datasets/' + filename\n",
    "\n",
    "            # Write the text file in the datasets folder\n",
    "            with open(path, 'w') as f:\n",
    "                f.write(texts)\n",
    "\n",
    "        else:\n",
    "            return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the text extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always an unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel ÄŒapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)\n\n"
    }
   ],
   "source": [
    "extract = Extract(return_as_file=False)\n",
    "\n",
    "# test_investopedia_texts = extract.extract_from_investopedia(['https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp'])\n",
    "# test_wikipedia_texts = extract.extract_from_wikipedia(['Artificial Intelligence'])\n",
    "\n",
    "print(test_wikipedia_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Model\n",
    "This function is used to finetune the model based on the current latest dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(dataset, any_checkpoint=False, reset_session=False):\n",
    "    '''\n",
    "\n",
    "    Function to finetune the model and save the trained model every checkpoint on the checkpoint folder.\n",
    "\n",
    "    @dataset: Path to the training data (TXT) with minimum 1024 tokens\n",
    "    @any_checkpoint: Boolean if there is any previous checkpoint\n",
    "    @reset_session: Boolean if reseting the session graph is needed\n",
    "\n",
    "    return: None\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Parameters\n",
    "    STEPS = 10000\n",
    "    MODEL_NAME = '355M'\n",
    "    LEARNING_RATE = 0.0001\n",
    "    RUN_NAME = 'trained_model'\n",
    "\n",
    "    MODEL_DIR = 'models'\n",
    "    CHECKPOINT_DIR = 'checkpoint'\n",
    "\n",
    "    # Clear session graph\n",
    "    if reset_session:\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    # Initialize training session\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    # Load the previous checkpoint\n",
    "    if any_checkpoint:\n",
    "        gpt2.load_gpt2(\n",
    "            sess,\n",
    "            run_name=RUN_NAME,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            model_name=None,\n",
    "            model_dir=MODEL_DIR,\n",
    "            multi_gpu=False\n",
    "        )\n",
    "\n",
    "    # Finetune the model\n",
    "    gpt2.finetune(\n",
    "        sess,\n",
    "        dataset=dataset,  # Dataset file\n",
    "        steps=STEPS,\n",
    "        model_name=MODEL_NAME,  # Model name: 124M, 355M, etc.\n",
    "        model_dir=MODEL_DIR,\n",
    "        combine=50000,\n",
    "        batch_size=1,\n",
    "        learning_rate=LEARNING_RATE,  # Learning rate\n",
    "        accumulate_gradients=5,\n",
    "        restore_from='latest',  # Start training the model from the latest model\n",
    "        run_name=RUN_NAME,  # Name of the trained model\n",
    "        checkpoint_dir=CHECKPOINT_DIR,  # Directory to save the model\n",
    "        sample_every=1000,\n",
    "        sample_length=300,  # Number of token generated\n",
    "        sample_num=1,\n",
    "        multi_gpu=False,\n",
    "        save_every=1000,\n",
    "        print_every=10,\n",
    "        max_checkpoints=1,\n",
    "        use_memory_saving_gradients=False,\n",
    "        only_train_transformer_layers=False,\n",
    "        optimizer='adam',\n",
    "        overwrite=True  # Overwrite the current model when training\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to test the finetuning model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_training_data = ''\n",
    "# finetune_model(test_training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "This functions is used to generate the text based on some input from the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(outline_to_length):\n",
    "    '''\n",
    "\n",
    "    Function to generate the text.\n",
    "\n",
    "    @outline_to_length: A 2D array containing the list of outline and the length desired\n",
    "        [[outline, length],\n",
    "        [outline, length],\n",
    "        [outline, length]]\n",
    "\n",
    "    return: List of generated text\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Parameters\n",
    "    MODEL_NAME = '355M'\n",
    "    RUN_NAME = 'trained_model'\n",
    "\n",
    "    MODEL_DIR = 'models'\n",
    "    CHECKPOINT_DIR = 'checkpoint'\n",
    "    SAMPLE_DIR = 'samples'\n",
    "\n",
    "    # Clear session graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Initialize TensorFlow session\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    # Create an empty list to store lists\n",
    "    essay = []\n",
    "\n",
    "    # Loop over the list\n",
    "    for record in outline_to_length:\n",
    "        prefix = record[0]  # The first sentence of the paragraph\n",
    "        length = record[1]  # The length of the paragraph (max: 1023)\n",
    "\n",
    "        text = gpt2.generate(\n",
    "            sess,\n",
    "            run_name=RUN_NAME,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            model_name=None,\n",
    "            model_dir=MODEL_DIR,\n",
    "            sample_dir=SAMPLE_DIR,\n",
    "            return_as_list=True,  # Return as list of string\n",
    "            truncate=None,\n",
    "            destination_path=None,\n",
    "            sample_delim='\\n\\n' + '=' * 20 + '\\n\\n',\n",
    "            prefix=prefix,\n",
    "            seed=None,\n",
    "            nsamples=1,  # Number of sample to be generated\n",
    "            batch_size=1,\n",
    "            length=length,\n",
    "            temperature=0.7,\n",
    "            top_k=0,\n",
    "            top_p=0.0,\n",
    "            include_prefix=True\n",
    "        )\n",
    "\n",
    "        text = ''.join(text) + '\\n\\n'\n",
    "        essay += text\n",
    "\n",
    "    return essay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_outline_to_length = [[]]\n",
    "# print(generate_text(test_outline_to_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing Text (Under Development)\n",
    "This function is used to paraphrase the sentences to avoid direct plagiarism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_paragraph(paragraphs):\n",
    "    '''\n",
    "\n",
    "    Function to paraphrase list of paragraphs.\n",
    "\n",
    "    @paragraph: A list containing the paragraphs\n",
    "\n",
    "    return: Paraphrased paragraphs\n",
    "\n",
    "    '''\n",
    "\n",
    "    def tag(sentence):\n",
    "        '''\n",
    "\n",
    "        Function to tag a word with their type.\n",
    "\n",
    "        @sentence: String sentence\n",
    "\n",
    "        return: List of words with their tags\n",
    "\n",
    "        '''\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sentence)\n",
    "        words = nltk.tag.pos_tag(words)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def paraphraseable(tag):\n",
    "        return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\n",
    "\n",
    "    def pos(tag):\n",
    "        if tag.startswith('NN'):\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "        elif tag.startswith('V'):\n",
    "            return nltk.corpus.wordnet.VERB\n",
    "\n",
    "    def synonyms(word, tag): \n",
    "        lemma_lists = [ss.lemmas() for ss in nltk.corpus.wordnet.synsets(word, pos(tag))]\n",
    "        lemmas = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "        return set(lemmas)\n",
    "\n",
    "    def if_synonym_exists(sentence):\n",
    "        for (word, t) in tag(sentence):\n",
    "            if paraphraseable(t):\n",
    "                syns = synonyms(word, t)\n",
    "                if syns:\n",
    "                    if len(syns) > 1:\n",
    "                        yield [word, list(syns)[1]]\n",
    "                        continue\n",
    "            yield [word, '']\n",
    "\n",
    "    def sentence_paraphrase(sentence):\n",
    "        return [w for w in if_synonym_exists(sentence)]\n",
    "\n",
    "    # 2D array of sentences in each paragraph\n",
    "    list_sentences = []\n",
    "\n",
    "    # Convert a list paragraph into lists of sentences\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = [s for s in nltk.tokenize.sent_tokenize(paragraph)]\n",
    "\n",
    "        # Loop over the sentences\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence_paraphrase(sentence)\n",
    "\n",
    "        list_sentences.append(sentences)\n",
    "\n",
    "    return list_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the paraphraser function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['At its core, AI is the branch of computer science that aims to answer Turing question in the affirmative.', 'It is the endeavor to replicate or simulate human intelligence in machines.'], ['The expansive goal of artificial intelligence has given rise to many questions and debates.', 'So much so, that no singular definition of the field is universally accepted.']]\n"
    }
   ],
   "source": [
    "test_paragraphs = ['At its core, AI is the branch of computer science that aims to answer Turing question in the affirmative. It is the endeavor to replicate or simulate human intelligence in machines.', 'The expansive goal of artificial intelligence has given rise to many questions and debates. So much so, that no singular definition of the field is universally accepted.']\n",
    "\n",
    "print(paraphrase_paragraph(test_paragraphs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Check (Under Development)\n",
    "This function is used to check and correct any grammatical error in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
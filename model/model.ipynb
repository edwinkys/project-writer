{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit382pyenv1518160d034e4ba6ab51bd99c9504900",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions\n",
    "\n",
    "This notebook contains the functions that is needed for the production model for the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Notebook\n",
    "Run all of the command below to start the notebook training session of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to kill the runtime in Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to install necessary file in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "# !pip install gpt_2_simple\n",
    "# !nvidia-smi\n",
    "# !mkdir -p datasets\n",
    "# !mkdir -p checkpoint\n",
    "# !mkdir -p samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to download the model in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt_2_simple as gpt2\n",
    "# gpt2.download_gpt2(model_name='124M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scrapping\n",
    "\n",
    "This function is used for scrapping all the text that is contains within a certain website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract():\n",
    "    def __init__(self, urls, return_as_file=True):\n",
    "        self.urls = urls  # List of urls for the dataset\n",
    "        self.return_as_file = return_as_file  # Export the dataset as a file\n",
    "\n",
    "    def extract_from_investopedia(self):\n",
    "        '''\n",
    "\n",
    "        Function to extract the text in the list of urls of Investopedia.\n",
    "\n",
    "        return: String containing text from urls\n",
    "\n",
    "        '''\n",
    "\n",
    "        # List of elements containing text\n",
    "        elements = [\n",
    "            'h1',\n",
    "            'h2',\n",
    "            'h3',\n",
    "            'h4',\n",
    "            'h5',\n",
    "            'h6',\n",
    "            'span',\n",
    "            'p'\n",
    "        ]\n",
    "\n",
    "        # Initialize string container\n",
    "        texts = ''\n",
    "\n",
    "        # Loop over the urls\n",
    "        for url in self.urls:\n",
    "            page = urlopen(url).read()\n",
    "\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "            list_text_tags = soup.find_all(elements)\n",
    "\n",
    "            for tag in list_text_tags:\n",
    "                texts += tag.text\n",
    "\n",
    "        if self.return_as_file:\n",
    "            filename = 'dataset_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())  # Export a text file\n",
    "            path = r'./datasets/' + filename\n",
    "\n",
    "            # Write the text file in the datasets folder\n",
    "            with open(path, 'w') as f:\n",
    "                f.write(texts)\n",
    "\n",
    "        else:\n",
    "            return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the text extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " and personal assistants such as Amazon's Alexa and Apple's Siri. You ask the assistant a question, it answers it for you.\n\nStrong artificial intelligence systems are systems that carry on the tasks considered to be human-like. These tend to be more complex and complicated systems. They are programmed to handle situations in which they may be required to problem solve without having a person intervene. These kinds of systems can be found in applications like self-driving cars or in hospital operating rooms.\n\nSince its beginning, artificial intelligence has come under scrutiny from scientists and the public alike. One common theme is the idea that machines will become so highly developed that humans will not be able to keep up and they will take off on their own, redesigning themselves at an exponential rate.\n\nAnother is that machines can hack into people's privacy and even be weaponized.Â Other arguments debate the ethics of artificial intelligence and whether intelligent systems such as robots should be treated with the same rights as humans.\n\nSelf-driving cars have been fairly controversial as their machines tend to be designed for the lowest possible risk and the least casualties. If presented with a scenario of colliding with one person or another at the same time, these cars would calculate the option that would cause the least amount of damage.\n\nAnother contentious issue many people have with artificial intelligence is how it may affect human employment. With many industries looking to automate certain jobs through the use of intelligent machinery, there is a concern that people would be pushed out of the workforce. Self-driving cars may remove the need for taxis and car-share programs, while manufacturers may easily replace human labor with machines, making people's skills more obsolete.\n\nContinuing Education\nAlgorithmic/Automated Trading Basic Education\nFinancial Technology & Automated Investing\nETF Trading Strategy & Education\nCompany Profiles\nCareer Advice\n"
    }
   ],
   "source": [
    "# Function test\n",
    "extract = Extract(['https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp'], return_as_file=False)\n",
    "test_texts = extract.extract_text()\n",
    "print(test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Model\n",
    "This function is used to finetune the model based on the current latest dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(dataset):\n",
    "    '''\n",
    "\n",
    "    Function to finetune the model and save the trained model every checkpoint on the checkpoint folder.\n",
    "\n",
    "    @dataset: Path to the training data (TXT) with minimum 1024 tokens\n",
    "    @model_name: The name of the model: 124M, 355M, etc.\n",
    "    @learning_rate: The learning rate of the model\n",
    "\n",
    "    return: None\n",
    "\n",
    "    '''\n",
    "    # Parameters\n",
    "    STEPS = 1000\n",
    "    MODEL_NAME = '124M'\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    # Clear session graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Initialize training session\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    # Finetune the model\n",
    "    gpt2.finetune(\n",
    "        sess,\n",
    "        dataset=dataset,  # Dataset CSV file\n",
    "        steps=STEPS,\n",
    "        model_name=MODEL_NAME,  # Model name: 124M, 355M, etc.\n",
    "        model_dir='models',\n",
    "        combine=50000,\n",
    "        batch_size=1,\n",
    "        learning_rate=LEARNING_RATE,  # Learning rate\n",
    "        accumulate_gradients=5,\n",
    "        restore_from='latest',  # Start training the model from the latest model\n",
    "        run_name='trained_model',  # Name of the trained model\n",
    "        checkpoint_dir='checkpoint',  # Directory to save the model\n",
    "        sample_every=250,\n",
    "        sample_length=500,  # Number of token generated\n",
    "        sample_num=1,\n",
    "        multi_gpu=False,\n",
    "        save_every=250,\n",
    "        print_every=10,\n",
    "        max_checkpoints=1,\n",
    "        use_memory_saving_gradients=False,\n",
    "        only_train_transformer_layers=False,\n",
    "        optimizer='adam',\n",
    "        overwrite=True  # Overwrite the current model when training\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to test the finetuning model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = ''\n",
    "# finetune_model(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "This functions is used to generate the text based on some input from the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(outline_to_length):\n",
    "    '''\n",
    "\n",
    "    Function to generate the text.\n",
    "\n",
    "    @outline_to_length: A 2D array containing the list of outline and the length desired\n",
    "        [[outline, length],\n",
    "        [outline, length],\n",
    "        [outline, length]]\n",
    "\n",
    "    return: List of generated text\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Clear session graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Initialize TensorFlow session\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    # Create an empty list to store lists\n",
    "    essay = []\n",
    "\n",
    "    # Loop over the list\n",
    "    for record in outline_to_length:\n",
    "        prefix = record[0]  # The first sentence of the paragraph\n",
    "        length = record[1]  # The length of the paragraph (max: 1023)\n",
    "\n",
    "        text = gpt2.generate(\n",
    "            sess,\n",
    "            run_name='trained_model',\n",
    "            checkpoint_dir='checkpoint',\n",
    "            model_name=None,\n",
    "            model_dir='models',\n",
    "            sample_dir='samples',\n",
    "            return_as_list=True,  # Return as list of string\n",
    "            truncate=None,\n",
    "            destination_path=None,\n",
    "            sample_delim='\\n' + '=' * 20 + '\\n\\n',\n",
    "            prefix=prefix,\n",
    "            seed=None,\n",
    "            nsamples=1,  # Number of sample to be generated\n",
    "            batch_size=1,\n",
    "            length=length,\n",
    "            temperature=0.7,\n",
    "            top_k=0,\n",
    "            top_p=0.0,\n",
    "            include_prefix=True\n",
    "        )\n",
    "\n",
    "        essay += text\n",
    "\n",
    "        # Add double newline\n",
    "        essay += ['\\n\\n']\n",
    "\n",
    "    return ''.join(essay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_outline_to_length = [[]]\n",
    "# print(generate_text(test_outline_to_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing Text\n",
    "This function is used to paraphrase the sentences to avoid direct plagiarism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Check\n",
    "This function is used to check and correct any grammatical error in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
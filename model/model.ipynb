{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit382pyenv1518160d034e4ba6ab51bd99c9504900",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions\n",
    "\n",
    "This notebook contains the functions that is needed for the production model for the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scrapping\n",
    "\n",
    "This function is used for scrapping all the text that is contains within a certain website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def extract_text(urls, export_as_file=True):\n",
    "    '''\n",
    "\n",
    "    Function to extract text from a website.\n",
    "\n",
    "    @urls: The list of website url\n",
    "    @export_as_file: Boolean to export the text result as a file\n",
    "\n",
    "    return: List of string or file containing the text\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def remove_blank_lines(paragraph):\n",
    "        '''\n",
    "\n",
    "        Function to remove extra blank lines in a paragraphs.\n",
    "\n",
    "        @paragraph: A list of string\n",
    "\n",
    "        return: A paragraph without extra blank lines\n",
    "\n",
    "        '''\n",
    "\n",
    "        lines = paragraph.split('\\n')\n",
    "\n",
    "        non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "\n",
    "        string_without_empty_lines = ''\n",
    "        for line in non_empty_lines:\n",
    "            string_without_empty_lines += line + '\\n'\n",
    "\n",
    "        return string_without_empty_lines\n",
    "\n",
    "\n",
    "    def clean_table_data(soup):\n",
    "        '''\n",
    "\n",
    "        Function to clean the table.\n",
    "\n",
    "        @soup: HTML page object from BeautifulSoup\n",
    "\n",
    "        return: Clean string containing table data\n",
    "\n",
    "        '''\n",
    "\n",
    "        table_elements = [\n",
    "        'table',\n",
    "        'thead',\n",
    "        'tbody',\n",
    "        'tfoot',\n",
    "        'tr',\n",
    "        'th',\n",
    "        'td'\n",
    "        ]\n",
    "\n",
    "        table_data = soup.find_all(table_elements, string=True)\n",
    "\n",
    "        string_table_data = ''\n",
    "        for data in table_data:\n",
    "            string_table_data += data.get_text() + ' '\n",
    "\n",
    "        return string_table_data\n",
    "\n",
    "\n",
    "    def delete_elements(soup, elements):\n",
    "        '''\n",
    "\n",
    "        Function to delete some elements.\n",
    "\n",
    "        @soup: HTML page object from BeautifulSoup\n",
    "        @elements: List of tags to delete\n",
    "\n",
    "        return: BeautifulSoup object without deleted elements\n",
    "\n",
    "        '''\n",
    "\n",
    "        for element in soup(elements):\n",
    "            element.decompose()\n",
    "\n",
    "        return soup\n",
    "\n",
    "    # List of elements to remove\n",
    "    ELEMENTS = [\n",
    "        'head',\n",
    "        'script',\n",
    "        'style',\n",
    "        'header',\n",
    "        'nav',\n",
    "        'table',\n",
    "        'form',\n",
    "        'input',\n",
    "        'button',\n",
    "        'footer'\n",
    "    ]\n",
    "\n",
    "    # Initialize the dataframe\n",
    "    df = DataFrame(columns=['texts'])\n",
    "\n",
    "    # Loop over the list of urls\n",
    "    for url in urls:\n",
    "        page = urlopen(url).read()\n",
    "\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        # Clean table\n",
    "        table_text = clean_table_data(soup)\n",
    "        soup = delete_elements(soup, ELEMENTS)\n",
    "\n",
    "        # Fetch the text from the soup\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Clean the text\n",
    "        text = text.strip()\n",
    "        text = remove_blank_lines(text)\n",
    "\n",
    "        # Append the text to the dataframe\n",
    "        new_record = {'texts': text}\n",
    "        df = df.append(new_record, ignore_index=True)\n",
    "\n",
    "    # Export dataframe\n",
    "    if export_as_file:\n",
    "        filename = 'dataset_{:%Y%m%d_%H%M%S}.csv'.format(datatime.utcnow())\n",
    "        path = r'/datasets/' + filename\n",
    "        df.to_csv(path, index=False, header=False)\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "texts\n0  Zeus\\nFrom Wikipedia, the free encyclopedia\\nJ...\n"
    }
   ],
   "source": [
    "# Function test\n",
    "url = ['https://simple.wikipedia.org/wiki/Zeus']\n",
    "print(extract_text(url, export_as_file=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Model\n",
    "This function is used to finetune the model based on the current latest dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to run in Google Colaboratory\n",
    "# %tensorflow_version 1.x\n",
    "# !pip install gpt_2_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "\n",
    "def finetune_model(dataset, model_name='124M', learning_rate=0.0001):\n",
    "    '''\n",
    "\n",
    "    Function to finetune the model and save the trained model every checkpoint on the checkpoint folder.\n",
    "\n",
    "    @dataset: Path to the training data (CSV)\n",
    "    @model_name: The name of the model: 124M, 355M, etc.\n",
    "    @learning_rate: The learning rate of the model\n",
    "\n",
    "    return: None\n",
    "\n",
    "    '''\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    gpt2.finetune(\n",
    "        sess,\n",
    "        dataset=dataset,  # Dataset CSV file\n",
    "        steps=-1,\n",
    "        model_name=model_name,  # Model name: 124M, 355M, etc.\n",
    "        model_dir='models',\n",
    "        combine=50000,\n",
    "        batch_size=1,\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        accumulate_gradients=5,\n",
    "        restore_from='latest',  # Start training the model from the latest model\n",
    "        run_name='trained_model',  # Name of the trained model\n",
    "        checkpoint_dir='checkpoint',  # Directory to save the model\n",
    "        sample_every=250,\n",
    "        sample_length=1023,  # Number of token generated\n",
    "        sample_num=1,\n",
    "        multi_gpu=False,\n",
    "        save_every=500,\n",
    "        print_every=50,\n",
    "        max_checkpoints=1,\n",
    "        use_memory_saving_gradients=False,\n",
    "        only_train_transformer_layers=False,\n",
    "        optimizer='adam',\n",
    "        overwrite=True  # Overwrite the current model when training\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "This functions is used to generate the text based on some input from the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(outline_to_length):\n",
    "    '''\n",
    "\n",
    "    Function to generate the text.\n",
    "\n",
    "    @outline_to_length: A 2D array containing the list of outline and the length desired\n",
    "        [[outline, length],\n",
    "        [outline, length],\n",
    "        [outline, length]]\n",
    "\n",
    "    return: List of generated text\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Initialize TensorFlow session\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    # Create an empty list to store lists\n",
    "    essay = []\n",
    "\n",
    "    # Loop over the list\n",
    "    for record in outline_to_length:\n",
    "        prefix = record[0]  # The first sentence of the paragraph\n",
    "        length = record[1]  # The length of the paragraph (max: 1023)\n",
    "\n",
    "        text = gpt2.generate(\n",
    "            sess,\n",
    "            run_name='trained_model',\n",
    "            checkpoint_dir='checkpoint',\n",
    "            model_name=None,\n",
    "            model_dir='models',\n",
    "            sample_dir='samples',\n",
    "            return_as_list=True,  # Return as list of string\n",
    "            truncate=None,\n",
    "            destination_path=None,\n",
    "            sample_delim='\\n' + '=' * 20 + '\\n\\n',\n",
    "            prefix=prefix,\n",
    "            seed=None,\n",
    "            nsamples=1,  # Number of sample to be generated\n",
    "            batch_size=1,\n",
    "            length=length,\n",
    "            temperature=0.7,\n",
    "            top_k=0,\n",
    "            top_p=0.0,\n",
    "            include_prefix=True\n",
    "        )\n",
    "\n",
    "        essay += text\n",
    "\n",
    "        # Add double newline\n",
    "        essay.append('\\n\\n')\n",
    "\n",
    "    return essay\n"
   ]
  }
 ]
}
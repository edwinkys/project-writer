{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit382pyenv1518160d034e4ba6ab51bd99c9504900",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class\n",
    "\n",
    "This notebook contains the AI model to finetune and generate the essay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Notebook\n",
    "Run all of the command below to start the notebook training session of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to kill the runtime in Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to install necessary file in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "\n",
    "# !pip install gpt_2_simple\n",
    "# !pip install wikipedia\n",
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/datasets\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/checkpoint\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to download the model in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt_2_simple as gpt2\n",
    "# gpt2.download_gpt2(model_name='355M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This class is used to finetune the model and generate text based on the current latest dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    '''\n",
    "\n",
    "    Class to finetune the model and generate the text based on the dataset.\n",
    "\n",
    "    @sess: GPT-2 TensorFlow session\n",
    "    @model_name: The name of the model: 124M, 355M, etc.\n",
    "    @run_name: The name of the trained model for each run\n",
    "    @model_dir: The path to the directory containing the model\n",
    "    @checkpoint_dir: The path to the directory containing the previously trained model\n",
    "    @sample_dir: The path to the directory to save the sample\n",
    "    @steps: Number of iteration to train the model\n",
    "    @learning_rate: The rate of learning of the model\n",
    "\n",
    "    Methods:\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Import libraries\n",
    "    import gpt_2_simple as gpt2\n",
    "    import tensorflow as tf\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sess,\n",
    "        model_name,\n",
    "        run_name,\n",
    "        model_dir,\n",
    "        checkpoint_dir,\n",
    "        sample_dir,\n",
    "        steps=30000,\n",
    "        learning_rate=0.0001\n",
    "    ):\n",
    "        self.sess = sess\n",
    "        self.model_name = model_name\n",
    "        self.run_name = run_name\n",
    "        self.model_dir = model_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.sample_dir = sample_dir\n",
    "        self.steps = steps\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def finetune_model(self, dataset):\n",
    "        '''\n",
    "        \n",
    "        Function to finetune the model and save the trained model every checkpoint on the checkpoint folder.\n",
    "\n",
    "        @sess: Tensorflow training session\n",
    "        @dataset: Path to the training dataset with minimum 1024 tokens\n",
    "\n",
    "        return: None\n",
    "\n",
    "        '''\n",
    "\n",
    "        gpt2.finetune(\n",
    "            self.sess,\n",
    "            dataset=dataset,  # Dataset file\n",
    "            steps=self.steps,\n",
    "            model_name=self.model_name,  # Model name: 124M, 355M, etc.\n",
    "            model_dir=self.model_dir,\n",
    "            combine=50000,\n",
    "            batch_size=1,\n",
    "            learning_rate=self.learning_rate,  # Learning rate\n",
    "            accumulate_gradients=5,\n",
    "            restore_from='latest',  # Start training the model from the latest model\n",
    "            run_name=self.run_name,  # Name of the trained model\n",
    "            checkpoint_dir=self.checkpoint_dir,  # Directory to save the model\n",
    "            sample_every=2500,\n",
    "            sample_length=300,  # Number of token generated\n",
    "            sample_num=1,\n",
    "            multi_gpu=False,\n",
    "            save_every=2500,\n",
    "            print_every=100,\n",
    "            max_checkpoints=1,\n",
    "            use_memory_saving_gradients=False,\n",
    "            only_train_transformer_layers=False,\n",
    "            optimizer='adam',\n",
    "            overwrite=True  # Overwrite the current model when training\n",
    "        )\n",
    "\n",
    "    def generate_text(self, outline_to_length, return_as_list=False):\n",
    "        '''\n",
    "\n",
    "        Function to generate the text.\n",
    "\n",
    "        @outline_to_length: A 2D array containing the list of outline and the length desired\n",
    "            [[outline, length],\n",
    "            [outline, length],\n",
    "            [outline, length]]\n",
    "\n",
    "        return: List of generated text\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Create an empty list to store lists\n",
    "        essay = []\n",
    "\n",
    "        # Loop over the list\n",
    "        for record in outline_to_length:\n",
    "            prefix = record[0]  # The first sentence of the paragraph\n",
    "            length = record[1]  # The length of the paragraph (max: 1023)\n",
    "\n",
    "            text = gpt2.generate(\n",
    "                self.sess,\n",
    "                run_name=self.run_name,\n",
    "                checkpoint_dir=self.checkpoint_dir,\n",
    "                model_name=None,\n",
    "                model_dir=self.model_dir,\n",
    "                sample_dir=self.sample_dir,\n",
    "                return_as_list=return_as_list,  # Return as list of string\n",
    "                truncate=None,\n",
    "                destination_path=None,\n",
    "                sample_delim='\\n\\n' + '=' * 10 + '\\n\\n',\n",
    "                prefix=prefix,\n",
    "                seed=None,\n",
    "                nsamples=1,  # Number of sample to be generated\n",
    "                batch_size=1,\n",
    "                length=length,  # Length of the generated text max: 1024 tokens\n",
    "                temperature=0.7,\n",
    "                top_k=0,\n",
    "                top_p=0.0,\n",
    "                include_prefix=True\n",
    "            )\n",
    "\n",
    "            text = ''.join(text) + '\\n\\n'\n",
    "            essay += text\n",
    "\n",
    "        return essay\n",
    "\n",
    "    def load_model(self):\n",
    "        '''\n",
    "\n",
    "        Function to load existing GPT-2 model.\n",
    "\n",
    "        return: None\n",
    "\n",
    "        '''\n",
    "\n",
    "        gpt2.load_gpt2(\n",
    "            self.sess,\n",
    "            run_name=self.run_name,\n",
    "            checkpoint_dir=self.checkpoint_dir,\n",
    "            model_name=None,\n",
    "            model_dir=self.model_dir,\n",
    "            multi_gpu=False,\n",
    "            reuse=True\n",
    "        )\n",
    "\n",
    "    def reset_session(self):\n",
    "        '''\n",
    "        \n",
    "        Function to reset the current TensorFlow session, to clear memory or load another model.\n",
    "\n",
    "        return: Reset session\n",
    "\n",
    "        '''\n",
    "\n",
    "        sess = gpt2.reset_session(self.sess)\n",
    "        return sess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "# Parameters\n",
    "SESS = gpt2.start_tf_sess()\n",
    "MODEL_NAME = '355M'\n",
    "RUN_NAME = ''\n",
    "MODEL_DIR = ''\n",
    "CHECKPOINT_DIR = ''\n",
    "SAMPLE_DIR = ''\n",
    "\n",
    "model = Model(\n",
    "    sess=SESS\n",
    "    model_name=MODEL_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    model_dir=MODEL_DIR,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    sample_dir=SAMPLE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = ''\n",
    "\n",
    "model.finetune(training_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_array = [\n",
    "    []\n",
    "]\n",
    "\n",
    "model.generate_text(paragraph_array, return_as_list=True)\n"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit382pyenv1518160d034e4ba6ab51bd99c9504900",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "This notebook contains the functions that is needed to scrap the dataset from various resources in the internet and export it as a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Notebook\n",
    "Run all of the command below to start the notebook training session of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to install necessary file in Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/datasets\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/samples\n",
    "# !mkdir -p drive/My\\ Drive/Project\\ Writer/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scrapping\n",
    "\n",
    "This class is used for scrapping all the text that is contains within a certain website and export it as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract():\n",
    "    '''\n",
    "\n",
    "    This class is used to extract text resources from web and export it as a file.\n",
    "\n",
    "    @return_as_file: Boolean to define whether the resources will be exported as a file.\n",
    "    \n",
    "    Method:\n",
    "    extract_from_investopedia\n",
    "    extract_from_wikipedia\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Import libraries\n",
    "    from urllib.request import urlopen\n",
    "    from bs4 import BeautifulSoup\n",
    "    from datetime import datetime\n",
    "    import wikipedia\n",
    "\n",
    "    def __init__(self, return_as_file=True):\n",
    "        self.return_as_file = return_as_file  # Export the dataset as a file\n",
    "\n",
    "    def remove_blank_lines(self, paragraph):\n",
    "        '''\n",
    "\n",
    "        Function to remove extra blank lines in a paragraphs.\n",
    "\n",
    "        @paragraph: A list of string\n",
    "\n",
    "        return: A paragraph without extra blank lines\n",
    "\n",
    "        '''\n",
    "\n",
    "        lines = paragraph.split('\\n')\n",
    "\n",
    "        non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "\n",
    "        string_without_empty_lines = ''\n",
    "        for line in non_empty_lines:\n",
    "            string_without_empty_lines += line + '\\n'\n",
    "\n",
    "        return string_without_empty_lines\n",
    "\n",
    "    def extract_from_investopedia(self, urls, dataset_dir=None):\n",
    "        '''\n",
    "\n",
    "        Function to extract the text in the list of urls of Investopedia.\n",
    "\n",
    "        @urls: List of investopedia urls\n",
    "        @dataset_dir: Path to the dataset directory: ./datasets/path/\n",
    "\n",
    "        return: String containing text from urls\n",
    "\n",
    "        '''\n",
    "\n",
    "        # List of elements containing text\n",
    "        elements = [\n",
    "            'article'\n",
    "        ]\n",
    "\n",
    "        # List of elements to delete\n",
    "        delete_elements = [\n",
    "            'header',\n",
    "            'span',\n",
    "            'footer'\n",
    "        ]\n",
    "\n",
    "        # Initialize string container\n",
    "        texts = ''\n",
    "\n",
    "        # Loop over the urls\n",
    "        for url in urls:\n",
    "            page = urlopen(url).read()\n",
    "\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "            # Delete some elements\n",
    "            for element in soup(delete_elements):\n",
    "                element.decompose()\n",
    "\n",
    "            # Remove useless div\n",
    "            for div in soup.find_all('div', ['breadcrumbs']): \n",
    "                div.decompose()\n",
    "\n",
    "            list_text_tags = soup.find_all(elements)\n",
    "\n",
    "            for tag in list_text_tags:\n",
    "                text = tag.text\n",
    "\n",
    "                # Remove extra spaces\n",
    "                text = text.strip()\n",
    "\n",
    "                # Add the text to the container\n",
    "                texts += text\n",
    "\n",
    "        # Remove extra spaces\n",
    "        texts = self.remove_blank_lines(texts)\n",
    "        texts = texts.strip()\n",
    "\n",
    "        if self.return_as_file:\n",
    "\n",
    "            filename = 'investopedia_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())  # Export a text file\n",
    "\n",
    "            if dataset_dir:\n",
    "                path = dataset_dir + filename\n",
    "\n",
    "            else:\n",
    "                path = r'./datasets/' + filename\n",
    "\n",
    "            # Write the text file in the datasets folder\n",
    "            with open(path, 'w') as f:\n",
    "                f.write(texts)\n",
    "\n",
    "        else:\n",
    "            return texts\n",
    "\n",
    "    def extract_from_wikipedia(self, titles, dataset_dir=None):\n",
    "        '''\n",
    "\n",
    "        Function to extract text from wikipedia.\n",
    "\n",
    "        @title: The list title of the Wikipedia article\n",
    "        @dataset_dir: Path to the directory dataset: ./datasets/path/\n",
    "\n",
    "        return: A string containing the text from a wikipedia\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Initialize the container\n",
    "        texts = ''\n",
    "\n",
    "        for title in titles:\n",
    "            # Get the wikipedia page\n",
    "            page = wikipedia.page(title)\n",
    "\n",
    "            # Extract the text\n",
    "            text = page.content\n",
    "\n",
    "            # Clean text\n",
    "            text = re.sub(r'==.*?==+', '', text)\n",
    "\n",
    "            texts += text\n",
    "        \n",
    "        texts = self.remove_blank_lines(texts)\n",
    "\n",
    "        if self.return_as_file:\n",
    "            filename = 'wikipedia_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())  # Export a text file\n",
    "\n",
    "            if dataset_dir:\n",
    "                path = dataset_dir + filename\n",
    "\n",
    "            else:\n",
    "                path = r'./datasets/' + filename\n",
    "\n",
    "            # Write the text file in the datasets folder\n",
    "            with open(path, 'w') as f:\n",
    "                f.write(texts)\n",
    "\n",
    "        else:\n",
    "            return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the text extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always an unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel ÄŒapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)\n\n"
    }
   ],
   "source": [
    "# extract = Extract(return_as_file=False)\n",
    "\n",
    "# test_investopedia_texts = extract.extract_from_investopedia(['https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp'])\n",
    "\n",
    "# test_wikipedia_texts = extract.extract_from_wikipedia(['Artificial Intelligence'])\n",
    "\n",
    "# print(test_wikipedia_texts)\n"
   ]
  }
 ]
}